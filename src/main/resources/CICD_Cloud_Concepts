Tools, CI/CD, and Cloud

1. What is Jenkins, and how does it fit in a CI/CD pipeline?
Answer:
Jenkins is an open-source automation server used to implement Continuous Integration (CI) and Continuous Deployment (CD).
In a CI/CD pipeline, Jenkins automatically:
Builds the code whenever changes are pushed to GitHub or GitLab,
Runs automated tests (like JUnit or integration tests),
And, if successful, deploys the build to environments like dev, QA, or production.
This ensures faster and more reliable delivery by automating the entire development-to-deployment process.

✅ In short: Jenkins automates build, test, and deployment to make software delivery faster and error-free.
========================================================================================================================
2. How do you configure a Git webhook to trigger a Jenkins build?
Answer:
To configure a Git webhook for Jenkins:
Go to your GitHub repository → Settings → Webhooks → Add Webhook.
Set the Payload URL to your Jenkins server’s Git endpoint, for example:
http://<jenkins-server-url>/github-webhook/
Choose “application/json” as the content type and select the push event trigger.
In Jenkins, create a Freestyle or Pipeline job, enable “GitHub hook trigger for GITScm polling”,
and connect it to your repo.
Now, whenever code is pushed to GitHub, the webhook automatically triggers a Jenkins build.
========================================================================================================================
3. Explain your project’s build and deployment process.
Answer:
In my project, the build and deployment process is automated through Jenkins:
Developer pushes code to the Git repository (GitHub).
A Jenkins pipeline is triggered via a webhook.
Jenkins executes Maven build commands (like mvn clean install) to compile and package the code.
JUnit and Mockito tests run automatically.
If the build passes, Jenkins creates a Docker image and pushes it to Docker Hub / ECR.
Finally, Jenkins deploys the container to a cloud server (e.g., AWS EC2 or ECS).

✅ This ensures every code change is built, tested, and deployed consistently and automatically.
========================================================================================================================
4. What AWS services have you used? (EC2, S3, Lambda, RDS, etc.)
Answer:
I’ve used several AWS services in my projects:
EC2: To host Spring Boot microservices.
S3: For storing files, logs, or backups.
RDS: For relational databases like MySQL or PostgreSQL.
CloudWatch: For monitoring metrics and application logs.
IAM: For managing user access and permissions securely.
ECR & ECS: For containerizing and deploying microservices using Docker images.

✅ These services together helped in achieving scalable, fault-tolerant, and secure deployments.
========================================================================================================================
5. How do you store application secrets securely in AWS?
Answer:
Application secrets (like DB passwords, API keys) are stored using AWS Secrets Manager or AWS Systems Manager Parameter
Store, not in code or configuration files.
In the application, Spring Boot can access them dynamically using:
spring.datasource.password=${aws.secretsmanager.db_password}
This ensures secrets are encrypted, rotated automatically, and securely accessed at runtime using IAM roles.

✅ Never hardcode credentials — always use encrypted secret stores.
========================================================================================================================
6. How do you monitor your application performance in production?
Answer:
We use AWS CloudWatch and Spring Boot Actuator for monitoring.
CloudWatch collects metrics like CPU usage, memory, and error logs.
Spring Boot Actuator exposes health, metrics, and custom monitoring endpoints.
For deeper insights, we integrate ELK Stack (Elasticsearch, Logstash, Kibana) or Prometheus + Grafana dashboards to
visualize API latency, error rates, and throughput.

✅ This helps in proactive alerting, quick debugging, and improving system performance.

========================================================================================================================
# Docker, Kafka, and Azure DevOps - Interview Questions and Answers

## Docker
1. **What is Docker, and why is it used?**
   Docker is a platform for containerizing applications, ensuring they run consistently across environments. It eliminates dependency issues and provides lightweight, portable solutions.

2. **How is a container different from a virtual machine?**
   Containers share the host OS kernel, making them lightweight and faster to start compared to VMs, which have a full OS layer.

3. **Explain Docker architecture.**
   - Docker Engine (runs containers).
   - Images (templates for containers).
   - Containers (runtime environments for applications).
   - Docker Hub (repository for images).

4. **What is the purpose of a Dockerfile?**
   It automates the image creation process by defining the instructions for setting up a container (e.g., installing dependencies, copying files).

5. **What are Docker volumes?**
   Volumes store data outside the container, ensuring persistence and enabling data sharing between containers.

6. **How do you scale applications with Docker?**
   Use Docker Compose or orchestration tools like Kubernetes to scale containers horizontally.

7. **How do you secure a Docker container?**
   - Use minimal base images.
   - Scan images for vulnerabilities.
   - Apply proper user permissions.
   - Avoid running containers as root.

8. **What are some common Docker commands?**
   - `docker build`, `docker run`, `docker ps`, `docker stop`, `docker rm`, `docker exec`.

---

## Kafka
1. **What is Kafka?**
   Kafka is a distributed event-streaming platform used for building real-time data pipelines and event-driven architectures.

2. **Explain Kafka's architecture.**
   Kafka has:
   - Producers (send messages).
   - Topics (logical channels for messages).
   - Consumers (read messages).
   - Brokers (Kafka servers storing and distributing messages).

3. **How is Kafka different from traditional message queues?**
   Kafka retains messages for a configurable time even after being read, allowing multiple consumers to process the same messages independently.

4. **What is a Kafka partition?**
   Topics are divided into partitions for scalability. Each partition is stored on a different broker, allowing parallel processing.

5. **What is the role of Zookeeper in Kafka?**
   Zookeeper manages metadata, tracks brokers, and coordinates leader elections. (In newer versions, Kafka has started removing Zookeeper dependencies).

6. **Explain Kafka’s replication mechanism.**
   Each partition has replicas for fault tolerance. A leader partition handles read/write, while followers replicate data for failover.

7. **What is a consumer group in Kafka?**
   A consumer group allows multiple consumers to share the workload of processing partitions in a topic.

8. **How do you monitor and troubleshoot Kafka?**
   Use tools like Prometheus, Grafana, or Kafka Manager to monitor metrics like broker health, lag, and throughput.

---

## Azure DevOps
1. **What is Azure DevOps?**
   Azure DevOps is a cloud-based platform offering CI/CD pipelines, version control, project management, and testing services.

2. **What are the key components of Azure DevOps?**
   - Azure Repos: Version control (Git).
   - Azure Pipelines: Build and release automation.
   - Azure Boards: Agile project management.
   - Azure Artifacts: Package management.
   - Azure Test Plans: Manual and automated testing.

3. **Explain how CI/CD works in Azure DevOps.**
   CI ensures code changes are automatically built and tested. CD automates deployment to staging or production. Pipelines define the process using YAML files or the GUI.

4. **How do you secure an Azure DevOps pipeline?**
   Use secure files, environment variables, role-based access control (RBAC), and secure service connections.

5. **What is a Release Pipeline in Azure DevOps?**
   A process that automates the deployment of applications to various environments (e.g., staging, production).

6. **How do you implement branching strategies in Azure Repos?**
   Use Git workflows like feature branching, Gitflow, or trunk-based development for collaboration and isolation.

7. **What are self-hosted agents in Azure DevOps?**
   Self-hosted agents run pipelines on your own infrastructure instead of Microsoft-hosted agents.

8. **How do you integrate Azure DevOps with other tools?**
   Use REST APIs, webhooks, or pre-built integrations with tools like Jira, Jenkins, and Kubernetes.
