1. What is Apache Kafka, and why is it used?
Answer:
    Apache Kafka is a distributed event streaming platform used for building real-time data pipelines and streaming applications.
    It handles massive amounts of real-time data.

    Use Cases:
        *Real-time event processing.
        *Decoupling services in microservices architecture.
        *Building scalable and fault-tolerant systems.
========================================================================================================================
2. Kafka Architecture Overview
Kafka’s architecture revolves around four main components: Producers, Consumers, Brokers, and Topics.

Here’s how data flows:
    Producer: Publishes messages to a topic.
    Consumer: read data from the topics.
    Topic: A category/channel to which messages are sent by producers and read by consumers.
    Broker: A Kafka server that stores, manage and distributes data.
    Zookeeper: Used for managing cluster metadata (deprecated in favor of Kafka Raft).
    Partition: Subdivides a topic for scalability.

Each topic is divided into partitions and distributed across brokers for parallelism.
You’ll typically have 3+ brokers for fault tolerance.
Replication factor = 3 ensures no data loss.
Partitions enable scalability and ordering.

Kafka guarantees high throughput, durability, and scalability—making it ideal for microservices that
rely on message-driven design.
========================================================================================================================
4. What is producer How do Kafka producers work?
A producer is a client that publishes records (messages) to a Kafka topic. Each message is assigned to a partition
based on a key or randomly. We can Also Define a Key or Groups. Send messages asynchronously to brokers.

Producers can configure acknowledgment strategies using the acks parameter:
     acks=0: No acknowledgment.
     acks=1: Acknowledgment from the leader.
     acks=all: Acknowledgment from all replicas.

example:

    Properties props = new Properties();
    props.put("bootstrap.servers", "localhost:9092");
    props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
    props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
    props.put("acks", "all");

    Producer<String, String> producer = new KafkaProducer<>(props);
    producer.send(new ProducerRecord<>("payment-events", "PaymentSuccess", "Order123"));
    producer.close();
========================================================================================================================
5. How do Kafka consumers work?
A Kafka consumer reads messages from topics and processes them in consumer groups. Each consumer in a group gets a
subset of partitions — ensuring load balancing and fault tolerance.

Features:
Offset Tracking: Consumers maintain offsets to know which messages they’ve read.
Rebalancing: When a consumer joins or leaves, Kafka redistributes partitions.
Parallelism: Multiple consumers increase throughput.
Kafka consumers subscribe to topics and poll for messages.
Each consumer belongs to a consumer group, and partitions are distributed among the group members.
If a consumer crashes, partitions are reassigned to other consumers in the group.

    example:
    Properties props = new Properties();
    props.put("bootstrap.servers", "localhost:9092");
    props.put("group.id", "payment-group");
    props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
    props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
    consumer.subscribe(Collections.singletonList("payment-events"));

    while (true) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
        for (ConsumerRecord<String, String> record : records) {
            System.out.println("Received: " + record.value());
        }
    }
========================================================================================================================
6. What is a Kafka topic, and how are messages stored in it?
Answer:

A category/channel to which messages are sent by producers and read by consumers. Each topic is devided into partitions, which
determine throughput and parallelism. Messages in a partition are ordered and stored with offsets.

Topic: Stream of messages
Partition: Ordered, immutable sequence
Offset: Message index within a partition
Parallelism: Each partition can be consumed independently.
Ordering: Guaranteed within a single partition.
Scalability: Add partitions to handle higher load.

Production Tips:
Don’t exceed 1000 partitions per broker.
Define partitions based on traffic patterns.
Use keys to maintain ordering guarantees.

Example:
In a payment system, a topic like payment-events can store events such as PaymentInitiated, PaymentSuccess, and PaymentFailed.
========================================================================================================================
7. Brokers & Clusters
Kafka brokers form the backbone of your Kafka cluster. Each broker stores partitions and handles read/write requests
from producers and consumers.

Production Considerations:
Minimum 3 brokers per cluster.
Use a replication factor of 3 for resilience when handling a Large amount of data, like log messages, etc.
Monitor broker health using Prometheus or Burrow.

Use rack awareness to distribute replicas across data centers.

========================================================================================================================
8. Schema Registry
In production, maintaining consistent data structures across microservices is critical. Schema Registry enforces
contracts between producers and consumers.

Benefits:
Prevents breaking changes.
Allows versioned evolution of message schemas.
Works with Avro, JSON Schema, and Protobuf.
You can use Confluent Schema Registry or Redpanda Schema Registry with KafkaJS using the @kafkajs/confluent-schema-registry package.
========================================================================================================================
9. How does Kafka achieve fault tolerance?
Answer:
    * Kafka achieves fault tolerance using replication.
    * Each partition has multiple replicas across brokers.
    * If a broker fails, a replica can take over as the leader for the partition.
========================================================================================================================
10. How does Kafka ensure message durability?
Answer:
    * Replication: Messages are replicated across brokers.
    * Commit Log: Messages are persisted to disk.
    * Acknowledgments: Producers receive acknowledgment based on acks configuration.

    Example:
    In a payment system, you can configure acks=all to ensure a message is acknowledged only when all replicas have received it.
========================================================================================================================
11. What is the role of partitions in Kafka?
Answer:
    * Partitions divide a topic for parallel processing.
    * Each partition has an independent offset sequence.
    * They allow Kafka to scale horizontally by distributing partitions across brokers.

========================================================================================================================

12. How do you achieve exactly-once delivery in Kafka?
Answer:
    Enable idempotent producers by setting enable.idempotence=true.
    Use Kafka Streams or transactions for atomic writes.

    example:
    props.put("enable.idempotence", "true");
========================================================================================================================
13. What is Kafka Streams, and how is it different from Kafka?
Answer:
    * Kafka Streams is a library for real-time stream processing.
    * It processes data from Kafka topics and outputs results to other topics or systems.
========================================================================================================================
14. What is the role of ZooKeeper in Kafka?
Answer:
    ZooKeeper manages metadata like:
        Broker information.
        Partition leader election.
        Consumer group coordination.

    Since Kafka 2.8, ZooKeeper is being replaced with Kafka Raft (KRaft) for metadata management.
========================================================================================================================
15. How do you handle backpressure in Kafka consumers?
Answer:
    Use pause() and resume() to control the flow of messages.
    Use a custom thread pool to process messages efficiently.

    example:
    if (processingLag > threshold) {
        consumer.pause(Collections.singletonList(new TopicPartition("payment-events", 0)));
    }
========================================================================================================================
16. How does Kafka handle message ordering?
    * Messages are ordered within a partition but not across partitions.
    * Use the same key to ensure all related messages go to the same partition.

========================================================================================================================
17. What is Kafka Connect?
    Kafka Connect is a tool for integrating Kafka with external systems like databases and storage.

        Source connectors: Import data into Kafka.
        Sink connectors: Export data from Kafka.
========================================================================================================================
18. What are Kafka offsets, and how are they managed?
    Offsets are unique identifiers for messages within a partition. They indicate the position of a message in the log.

        Kafka tracks offsets at the consumer group level for each partition.
========================================================================================================================
19. How does Kafka achieve exactly-once delivery semantics?
    Kafka achieves exactly-once semantics through:

    1. Idempotent Producers: Ensures no duplicate messages are written, even during retries (enable.idempotence=true).
    2. Kafka Transactions: Allows atomic writes across multiple partitions and offsets.
    3. Consumer Side Deduplication: Ensures no duplicate processing by storing processed event IDs.

    These features together prevent duplicates in both publishing and consuming messages.
========================================================================================================================
20. What is log compaction, and when would you use it?
    Log compaction is a Kafka feature that retains only the latest value for each key in a topic.
    Older versions are deleted.

    Use cases:
    Maintaining a system's current state (e.g., user profiles, configuration updates).
    Reducing storage by eliminating outdated data.
========================================================================================================================
21. Security (SASL, SSL, ACLs)
    Security Layers:

    SASL/SSL Authentication: Ensures secure identity verification.
    Authorization (ACLs): Controls access to topics.
    Encryption (TLS): Protects data in transit

========================================================================================================================
    Error Handling & Retry Logic
    In real-world production systems, things fail — brokers go down, messages get corrupted, or network issues arise.

    Best Practices:

    Retry Policy: Use exponential backoff.
    Dead Letter Queue (DLQ): Capture failed messages.
    Idempotent Producers: Prevent duplicates on retry.

========================================================================================================================
    Poison Message Handling: Skip or park malformed events.

    Article content
    Monitoring & Logging
    Monitoring Stack:

    Prometheus: Metrics collection
    Grafana: Visualization
    Burrow: Lag monitoring
    ELK Stack: Log aggregation
     Performance Tuning
========================================================================================================================

    Optimize Kafka for large-scale Node.js production systems:

    Enable compression (lz4/snappy).
    Tune batch.size and linger.ms for producers.
    Increase fetch.max.bytes for consumers.
    Use async/await instead of blocking loops.

========================================================================================================================

    Consumer lag
    Broker disk usage
    Partition under-replication
    Message throughput (MB/s)
========================================================================================================================
    Message Key
    The message key determines which partition a record belongs to. It also ensures the ordering of related messages.

    Example:

    Key = user123 → All events for this user go to the same partition.
    No key = random partition assignment.
========================================================================================================================
    Serialization
    Serialization converts structured data into a byte stream for transmission. Kafka supports multiple formats:

    JSON: Simple and readable.
    Avro: Compact and schema-based.
    Protobuf: Language-neutral and version-safe.






